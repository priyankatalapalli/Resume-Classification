{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import pickle\n",
    "import streamlit as st\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import streamlit.components.v1 as components\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import win32com.client as win32\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "pickle_in = open('classifier.pkl', 'rb')\n",
    "classifier = pickle.load(pickle_in)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "result = string.punctuation\n",
    "ps = PorterStemmer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# Cleaning the data\n",
    "def resumecleaning(resume):\n",
    "    resume = re.sub(r'http\\S+', '', resume)\n",
    "    resume = word_tokenize(resume.lower())\n",
    "    resume = [word for word in resume if word not in stop and word not in result]\n",
    "    resume = [ps.stem(word) for word in resume]\n",
    "    resume = nlp(' '.join(resume))\n",
    "    resume = [token.lemma_ for token in resume]\n",
    "    cleaned_text = ' '.join(resume)\n",
    "    return cleaned_text\n",
    "\n",
    "def display():\n",
    "    st.title(\"Resume - Classifier\")\n",
    "\n",
    "    # File upload section\n",
    "    uploaded_file = st.file_uploader(\"Upload your resume\", type=[\"pdf\", \"docx\", \"doc\"])\n",
    "\n",
    "    # Checking the type of file was uploaded and extracting it to for further processing\n",
    "    if uploaded_file:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "            temp_file.write(uploaded_file.read())\n",
    "\n",
    "        if uploaded_file.name.endswith('.docx'):\n",
    "            resume_text = docx2txt.process(temp_file.name)\n",
    "        elif uploaded_file.name.endswith('.doc'):\n",
    "            try:\n",
    "                word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "                doc = word.Documents.Open(os.path.abspath(temp_file.name))\n",
    "                resume_text = doc.Content.Text\n",
    "                doc.Close()\n",
    "                word.Quit()\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {e}\")\n",
    "                return\n",
    "        elif uploaded_file.name.endswith('.pdf'):\n",
    "            reader = PyPDF2.PdfReader(temp_file.name)\n",
    "            resume_text = \"\"\n",
    "            for page in reader.pages:\n",
    "                resume_text += page.extract_text()\n",
    "        else:\n",
    "            st.warning(\"Invalid file format. Please upload a PDF or Word document.\")\n",
    "            return\n",
    "        \n",
    "        # Load the trained vectorizer\n",
    "        with open('vectorizer.pkl', 'rb') as vectorizer_file:\n",
    "            vectorizer = pickle.load(vectorizer_file)\n",
    "        \n",
    "        # Cleaning the uploaded resume text\n",
    "        cleaned = resumecleaning(resume_text)\n",
    "\n",
    "        # Vectorize the cleaned resume text using the loaded vectorizer\n",
    "        cleaned_resume_vectorized = vectorizer.transform([cleaned])\n",
    "\n",
    "        # Make predictions\n",
    "        predicted_probabilities = classifier.predict_proba(cleaned_resume_vectorized)[0]\n",
    "\n",
    "        # Define the output classes\n",
    "        output_classes = ['Peoplesoft', 'React', 'SQL', 'workday']\n",
    "\n",
    "        # Display the predicted classes with probability greater than 0.5\n",
    "        # st.write(\"The classified resume belong to the category of  \")\n",
    "        for class_name, prediction_value in zip(output_classes, predicted_probabilities):\n",
    "            if prediction_value > 0.5:\n",
    "                st.markdown(f\"<h3 style='font-size: 24px;'>The uploaded Resume belongs to the category of {class_name} Resumes</h3>\", unsafe_allow_html=True)\n",
    "\n",
    "    else:\n",
    "        st.warning(\"Please upload a file.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    display()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
